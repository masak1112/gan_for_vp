{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import layer_def as ld\n",
    "import BasicConvLSTMCell\n",
    "#from BasicConvLSTMCell import BasicConvLSTMCell\n",
    "from mcnet_ops import *\n",
    "from mcnet_utils import *\n",
    "importlib.reload(ld)\n",
    "importlib.reload(BasicConvLSTMCell)\n",
    "from BasicConvLSTMCell import BasicConvLSTMCell\n",
    "train_files = \"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/train\"\n",
    "test_files = \"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/test\"\n",
    "val_files = \"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/train\"\n",
    "#hyparameters\n",
    "num_epochs = 50\n",
    "input_dim=3\n",
    "num_sample = 1500\n",
    "alpha = 1\n",
    "beta = 0.02\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(type=\"train\",batch_size=batch_size):\n",
    "    if type==\"train\": filenames = glob.glob(\"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/train/*.tfrecords\")\n",
    "    if type==\"val\":filenames = glob.glob(\"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/val/*.tfrecords\")\n",
    "    if type==\"test\":filenames = glob.glob(\"/Users/gongbing/PycharmProjects/video_prediction_savp/data/era5_size_64_64_3_3t_norm/test/*.tfrecords\")\n",
    "    def parser(serialized_example):\n",
    "            seqs = OrderedDict()\n",
    "            keys_to_features = {\n",
    "                # 'width': tf.FixedLenFeature([], tf.int64),\n",
    "                # 'height': tf.FixedLenFeature([], tf.int64),\n",
    "                'sequence_length': tf.FixedLenFeature([], tf.int64),\n",
    "                # 'channels': tf.FixedLenFeature([],tf.int64),\n",
    "                # 'images/encoded':  tf.FixedLenFeature([], tf.string)\n",
    "                'images/encoded': tf.VarLenFeature(tf.float32)\n",
    "            }\n",
    "            parsed_features = tf.parse_single_example(serialized_example, keys_to_features)\n",
    "            seq = tf.sparse_tensor_to_dense(parsed_features[\"images/encoded\"])\n",
    "            images = tf.reshape(seq, [20,64, 64,3], name = \"reshape_new\")\n",
    "            seqs[\"images\"] = images\n",
    "            return seqs\n",
    "    dataset = tf.data.TFRecordDataset(filenames, buffer_size = 8 * 1024 * 1024)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "            parser, batch_size, drop_remainder = True, num_parallel_calls = None))\n",
    "    \n",
    "    dataset = dataset.prefetch(batch_size) \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCNET(object):\n",
    "    def __init__(self, learning_rate=1e-2, image_size=[64,64], batch_size=32, c_dim=3,\n",
    "                 checkpoint_dir=None, is_train=True,context_frames=10,sequence_length=20,model_name=\"mcnet\"):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.model_name = model_name\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.gf_dim = 64\n",
    "        self.df_dim = 64\n",
    "\n",
    "        self.c_dim = c_dim\n",
    "        self.context_frames = context_frames # context_frames=10 conresponds to K\n",
    "        self.K = self.context_frames\n",
    "        self.sequence_length = 20\n",
    "        self.predict_frames = sequence_length - context_frames #predict_frames corresponds to T\n",
    "        self.T = self.predict_frames\n",
    "        \n",
    "        self.diff_shape = [self.batch_size, self.context_frames-1, self.image_size[0],\n",
    "                           self.image_size[1], c_dim]\n",
    "        self.target_shape = [self.batch_size, self.sequence_length, self.image_size[0], self.image_size[1],c_dim]\n",
    "        self.xt_shape = [self.batch_size, self.image_size[0], self.image_size[1], c_dim]\n",
    "        print(\"target shape inti:\",self.target_shape)\n",
    "\n",
    "        self.build()\n",
    "        \n",
    "        # Initialize paramters\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Summary op\n",
    "        #self.loss_summary = tf.summary.scalar(\"total_losses\", self.total_loss)\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.summary_dir = \"./\"\n",
    "        self.base_dir = model_name\n",
    "        self.checkpoint_dir = self.base_dir + \"/checkpoint\"\n",
    "        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "        self.train_log_file = self.base_dir + \"/train_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.val_log_file = self.base_dir + \"/val_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.train_writer = tf.summary.FileWriter(self.train_log_file, self.sess.graph)\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_file, self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(12345)\n",
    "        self.train_iterator = make_dataset(type=\"train\")\n",
    "        self.val_iterator = make_dataset(type=\"val\")\n",
    "        self.test_iterator = make_dataset(type=\"test\")\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        #ARCHITECTURE\n",
    "        self.x = tf.placeholder(tf.float32, self.target_shape, name='target')\n",
    "  \n",
    "        #self.xt = tf.placeholder(tf.float32, self.xt_shape, name='xt')\n",
    "        self.xt = self.x[:,self.context_frames-1,:,:,:]\n",
    "\n",
    "        self.diff_in = tf.placeholder(tf.float32, self.diff_shape, name='diff_in')\n",
    "        diff_in_all = []\n",
    "        for t in range(1,self.context_frames):\n",
    "            prev = self.x[:,t-1,:,:,:]\n",
    "            next = self.x[:,t,:,:,:]\n",
    "            diff_in = tf.reshape(next - prev,[self.batch_size,1,self.image_size[0],self.image_size[1],-1])\n",
    "            diff_in_all.append(diff_in)\n",
    " \n",
    "        self.diff_in = tf.concat(axis=1,values=diff_in_all)\n",
    " \n",
    "            \n",
    "        \n",
    "        cell = BasicConvLSTMCell([self.image_size[0]/8, self.image_size[1]/8],[3, 3], 256)\n",
    "    \n",
    "        pred = self.forward(self.diff_in, self.xt, cell)\n",
    "      \n",
    "        #Bing++++++++++\n",
    "        #self.G = tf.concat(axis=3,values=pred)\n",
    "        self.G = tf.concat(axis=1,values=pred)\n",
    "        #Bing---------\n",
    "        if self.is_train:\n",
    "            #Bing+++++++++++++\n",
    "            #true_sim = inverse_transform(self.target[:,:,:,self.K:,:])\n",
    "            true_sim = self.x[:,self.K:,:,:,:]\n",
    "            #Bing--------------\n",
    "            #Bing: the following make sure the channel is three dimension, if the channle is 3 then will be duplicated\n",
    "            if self.c_dim == 1: true_sim = tf.tile(true_sim,[1,1,1,1,3])\n",
    "            #Bing+++++++++++++\n",
    "            #Bing: the raw inputs shape is [batch_size, image_size[0],self.image_size[1], num_seq, channel]. tf.transpose will transpoe the shape into \n",
    "            #[batch size*num_seq, image_size0, image_size1, channels], for our era5 case, we do not need transpose\n",
    "            #true_sim = tf.reshape(tf.transpose(true_sim,[0,3,1,2,4]),\n",
    "            #                             [-1, self.image_size[0],\n",
    "            #                              self.image_size[1], 3])\n",
    "            true_sim = tf.reshape(true_sim,[-1, self.image_size[0],self.image_size[1], 3]) \n",
    "            #Bing--------------\n",
    "            \n",
    "        #Bing+++++++++++++    \n",
    "        #gen_sim = inverse_transform(self.G)\n",
    "        gen_sim = self.G\n",
    "    \n",
    "        if self.c_dim == 1: gen_sim = tf.tile(gen_sim,[1,1,1,1,3])\n",
    "        #gen_sim = tf.reshape(tf.transpose(gen_sim,[0,3,1,2,4]),\n",
    "        #                                [-1, self.image_size[0],\n",
    "        #                                self.image_size[1], 3])\n",
    "        \n",
    "        gen_sim = tf.reshape(gen_sim,[-1, self.image_size[0],self.image_size[1], 3])\n",
    "        \n",
    "        #Bing+++++++++++++ \n",
    "        #Bing:the shape of the layer will be channels*num_seq, why ?   \n",
    "        #binput = tf.reshape(self.target[:,:,:,:self.K,:],\n",
    "        #                  [self.batch_size, self.image_size[0],\n",
    "        #                   self.image_size[1], -1])\n",
    "        binput = tf.reshape(tf.transpose(self.x[:,:self.K,:,:,:],[0,1,2,3,4]),\n",
    "                          [self.batch_size, self.image_size[0],\n",
    "                           self.image_size[1], -1])\n",
    "        #Bing--------------\n",
    "        btarget = tf.reshape(tf.transpose(self.x[:,self.K:,:,:,:],[0,1,2,3,4]),\n",
    "                           [self.batch_size, self.image_size[0],\n",
    "                            self.image_size[1], -1])\n",
    "        bgen = tf.reshape(self.G,[self.batch_size,\n",
    "                                self.image_size[0],\n",
    "                                self.image_size[1], -1])\n",
    "        \n",
    "        good_data = tf.concat(axis=3,values=[binput,btarget])\n",
    "        gen_data  = tf.concat(axis=3,values=[binput,bgen])\n",
    "        self.gen_data = gen_data\n",
    "\n",
    "        with tf.variable_scope(\"DIS\", reuse=False):\n",
    "            self.D, self.D_logits = self.discriminator(good_data)\n",
    "\n",
    "        with tf.variable_scope(\"DIS\", reuse=True):\n",
    "            self.D_, self.D_logits_ = self.discriminator(gen_data)\n",
    "        \n",
    "        self.L_p = tf.reduce_mean(\n",
    "          tf.square(self.G-self.x[:,self.K:,:,:,:]))\n",
    "        \n",
    "        self.L_gdl = gdl(gen_sim, true_sim, 1.)\n",
    "        self.L_img = self.L_p + self.L_gdl\n",
    "\n",
    "        self.d_loss_real = tf.reduce_mean(\n",
    "          tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "              logits=self.D_logits, labels=tf.ones_like(self.D)\n",
    "          ))\n",
    "        self.d_loss_fake = tf.reduce_mean(\n",
    "          tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "              logits=self.D_logits_, labels=tf.zeros_like(self.D_)\n",
    "          ))\n",
    "        self.d_loss = self.d_loss_real + self.d_loss_fake\n",
    "        self.L_GAN = tf.reduce_mean(\n",
    "          tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "              logits=self.D_logits_, labels=tf.ones_like(self.D_)\n",
    "          ))\n",
    "\n",
    "        self.loss_sum = tf.summary.scalar(\"L_img\", self.L_img)\n",
    "        self.L_p_sum = tf.summary.scalar(\"L_p\", self.L_p)\n",
    "        self.L_gdl_sum = tf.summary.scalar(\"L_gdl\", self.L_gdl)\n",
    "        self.L_GAN_sum = tf.summary.scalar(\"L_GAN\", self.L_GAN)\n",
    "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        self.d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", self.d_loss_real)\n",
    "        self.d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", self.d_loss_fake)\n",
    "        #Bing ++++++++\n",
    "        self.total_loss = alpha*self.L_img+beta*self.L_GAN\n",
    "        self.total_loss_sum = tf.summary.scalar(\"total_loss\", self.total_loss)\n",
    "        self.g_sum = tf.summary.merge([self.L_p_sum,\n",
    "                              self.L_gdl_sum, self.loss_sum,\n",
    "                              self.L_GAN_sum])\n",
    "        self.d_sum = tf.summary.merge([self.d_loss_real_sum, self.d_loss_sum,\n",
    "                              self.d_loss_fake_sum])\n",
    "     \n",
    "        #Bing ---------\n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.g_vars = [var for var in self.t_vars if 'DIS' not in var.name]\n",
    "        self.d_vars = [var for var in self.t_vars if 'DIS' in var.name]\n",
    "        num_param = 0.0\n",
    "        for var in self.g_vars:\n",
    "            num_param += int(np.prod(var.get_shape()));\n",
    "        print(\"Number of parameters: %d\"%num_param)\n",
    "        #Bing ++++++++\n",
    "        #self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        #Bing --------\n",
    "\n",
    "\n",
    "        #Training\n",
    "        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5).minimize(\n",
    "        self.d_loss, var_list=self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5).minimize(\n",
    "            alpha*self.L_img+beta*self.L_GAN, var_list=self.g_vars)\n",
    "    \n",
    "\n",
    "    def forward(self, diff_in, xt, cell):\n",
    "        # Initial state\n",
    "        state = tf.zeros([self.batch_size, self.image_size[0]/8,\n",
    "                      self.image_size[1]/8, 512])\n",
    "        reuse = False\n",
    "        # Encoder\n",
    "        #Bing++++++++++++++++++++++++++++\n",
    "        for t in range(self.K-1):\n",
    "            \n",
    "            enc_h, res_m = self.motion_enc(diff_in[:,t,:,:,:], reuse=reuse)\n",
    "            h_dyn, state = cell(enc_h, state, scope='lstm', reuse=reuse)\n",
    "            reuse = True\n",
    "        pred = []\n",
    "       # Decoder\n",
    "        for t in range(self.T):\n",
    "            if t == 0:\n",
    "                h_cont, res_c = self.content_enc(xt, reuse=False)\n",
    "                h_tp1 = self.comb_layers(h_dyn, h_cont, reuse=False)\n",
    "                res_connect = self.residual(res_m, res_c, reuse=False)\n",
    "                x_hat = self.dec_cnn(h_tp1, res_connect, reuse=False)\n",
    "             \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                enc_h, res_m = self.motion_enc(diff_in, reuse=True)\n",
    "                h_dyn, state = cell(enc_h, state, scope='lstm', reuse=True)\n",
    "                h_cont, res_c = self.content_enc(xt, reuse=reuse)\n",
    "                h_tp1 = self.comb_layers(h_dyn, h_cont, reuse=True)\n",
    "                res_connect = self.residual(res_m,res_c, reuse=True)\n",
    "                x_hat = self.dec_cnn(h_tp1, res_connect, reuse=True)\n",
    "        \n",
    "          \n",
    "           \n",
    "            x_hat_gray = x_hat\n",
    "            xt_gray = xt\n",
    "          \n",
    "            diff_in = x_hat_gray - xt_gray\n",
    "            xt = x_hat\n",
    "            #Bing++++++++++++++++++++++++++++\n",
    "            #pred.append(tf.reshape(x_hat,[self.batch_size, self.image_size[0],\n",
    "            #                        self.image_size[1], 1, self.c_dim]))\n",
    "            pred.append(tf.reshape(x_hat,[self.batch_size,1, self.image_size[0],\n",
    "                                    self.image_size[1], self.c_dim]))\n",
    "             #Bing----------------\n",
    "        return pred\n",
    "\n",
    "    def motion_enc(self, diff_in, reuse):\n",
    "        res_in = []\n",
    "        \n",
    "        conv1 = relu(conv2d(diff_in, output_dim=self.gf_dim, k_h=5, k_w=5,\n",
    "                            d_h=1, d_w=1, name='dyn1_conv1', reuse=reuse))\n",
    "        res_in.append(conv1)\n",
    "        pool1 = MaxPooling(conv1, [2,2])\n",
    "\n",
    "        conv2 = relu(conv2d(pool1, output_dim=self.gf_dim*2, k_h=5, k_w=5,\n",
    "                            d_h=1, d_w=1, name='dyn_conv2',reuse=reuse))\n",
    "        res_in.append(conv2)\n",
    "        pool2 = MaxPooling(conv2, [2,2])\n",
    "\n",
    "        conv3 = relu(conv2d(pool2, output_dim=self.gf_dim*4, k_h=7, k_w=7,\n",
    "                            d_h=1, d_w=1, name='dyn_conv3',reuse=reuse))\n",
    "        res_in.append(conv3)\n",
    "        pool3 = MaxPooling(conv3, [2,2])\n",
    "        return pool3, res_in\n",
    "\n",
    "    def content_enc(self, xt, reuse):\n",
    "        res_in  = []\n",
    "        conv1_1 = relu(conv2d(xt, output_dim=self.gf_dim, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv1_1',reuse=reuse))\n",
    "        conv1_2 = relu(conv2d(conv1_1, output_dim=self.gf_dim, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv1_2',reuse=reuse))\n",
    "        res_in.append(conv1_2)\n",
    "        pool1 = MaxPooling(conv1_2, [2,2])\n",
    "\n",
    "        conv2_1 = relu(conv2d(pool1, output_dim=self.gf_dim*2, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv2_1',reuse=reuse))\n",
    "        conv2_2 = relu(conv2d(conv2_1, output_dim=self.gf_dim*2, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv2_2',reuse=reuse))\n",
    "        res_in.append(conv2_2)\n",
    "        pool2 = MaxPooling(conv2_2, [2,2])\n",
    "\n",
    "        conv3_1 = relu(conv2d(pool2, output_dim=self.gf_dim*4, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv3_1', reuse=reuse))\n",
    "        conv3_2 = relu(conv2d(conv3_1, output_dim=self.gf_dim*4, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv3_2', reuse=reuse))\n",
    "        conv3_3 = relu(conv2d(conv3_2, output_dim=self.gf_dim*4, k_h=3, k_w=3,\n",
    "                              d_h=1, d_w=1, name='cont_conv3_3',reuse=reuse))\n",
    "        res_in.append(conv3_3)\n",
    "        pool3 = MaxPooling(conv3_3, [2,2])\n",
    "        return pool3, res_in\n",
    "\n",
    "    def comb_layers(self, h_dyn, h_cont, reuse=False):\n",
    "        comb1 = relu(conv2d(tf.concat(axis=3,values=[h_dyn, h_cont]),\n",
    "                            output_dim=self.gf_dim*4, k_h=3, k_w=3,\n",
    "                            d_h=1, d_w=1, name='comb1',reuse=reuse))\n",
    "        comb2 = relu(conv2d(comb1, output_dim=self.gf_dim*2, k_h=3, k_w=3,\n",
    "                            d_h=1, d_w=1, name='comb2', reuse=reuse))\n",
    "        h_comb = relu(conv2d(comb2, output_dim=self.gf_dim*4, k_h=3, k_w=3,\n",
    "                             d_h=1, d_w=1, name='h_comb', reuse=reuse))\n",
    "        return h_comb\n",
    "\n",
    "    def residual(self, input_dyn, input_cont, reuse=False):\n",
    "        n_layers = len(input_dyn)\n",
    "        res_out = []\n",
    "        for l in range(n_layers):\n",
    "            input_ = tf.concat(axis=3,values=[input_dyn[l],input_cont[l]])\n",
    "            out_dim = input_cont[l].get_shape()[3]\n",
    "            res1 = relu(conv2d(input_, output_dim=out_dim,\n",
    "                             k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                             name='res'+str(l)+'_1', reuse=reuse))\n",
    "            res2 = conv2d(res1, output_dim=out_dim, k_h=3, k_w=3,\n",
    "                        d_h=1, d_w=1, name='res'+str(l)+'_2', reuse=reuse)\n",
    "            res_out.append(res2)\n",
    "        return res_out\n",
    "\n",
    "    def dec_cnn(self, h_comb, res_connect, reuse=False):\n",
    "       \n",
    "        \n",
    "        shapel3 = [self.batch_size, int(self.image_size[0]/4),\n",
    "                   int(self.image_size[1]/4), self.gf_dim*4]\n",
    "        shapeout3 = [self.batch_size, int(self.image_size[0]/4),\n",
    "                     int(self.image_size[1]/4), self.gf_dim*2]\n",
    "        depool3 = FixedUnPooling(h_comb, [2,2])\n",
    "        deconv3_3 = relu(deconv2d(relu(tf.add(depool3, res_connect[2])),\n",
    "                                  output_shape=shapel3, k_h=3, k_w=3,\n",
    "                                  d_h=1, d_w=1, name='dec_deconv3_3', reuse=reuse))\n",
    "        deconv3_2 = relu(deconv2d(deconv3_3, output_shape=shapel3, k_h=3, k_w=3,\n",
    "                                  d_h=1, d_w=1, name='dec_deconv3_2', reuse=reuse))\n",
    "        deconv3_1 = relu(deconv2d(deconv3_2, output_shape=shapeout3, k_h=3, k_w=3,\n",
    "                                  d_h=1, d_w=1, name='dec_deconv3_1', reuse=reuse))\n",
    "\n",
    "        shapel2 = [self.batch_size, int(self.image_size[0]/2),\n",
    "                   int(self.image_size[1]/2), self.gf_dim*2]\n",
    "        shapeout3 = [self.batch_size, int(self.image_size[0]/2),\n",
    "                     int(self.image_size[1]/2), self.gf_dim]\n",
    "        depool2 = FixedUnPooling(deconv3_1, [2,2])\n",
    "        deconv2_2 = relu(deconv2d(relu(tf.add(depool2, res_connect[1])),\n",
    "                                  output_shape=shapel2, k_h=3, k_w=3,\n",
    "                                  d_h=1, d_w=1, name='dec_deconv2_2', reuse=reuse))\n",
    "        deconv2_1 = relu(deconv2d(deconv2_2, output_shape=shapeout3, k_h=3, k_w=3,\n",
    "                                  d_h=1, d_w=1, name='dec_deconv2_1', reuse=reuse))\n",
    "\n",
    "        shapel1 = [self.batch_size, self.image_size[0],\n",
    "                   self.image_size[1], self.gf_dim]\n",
    "        shapeout1 = [self.batch_size, self.image_size[0],\n",
    "                     self.image_size[1], self.c_dim]\n",
    "        depool1 = FixedUnPooling(deconv2_1, [2,2])\n",
    "        deconv1_2 = relu(deconv2d(relu(tf.add(depool1, res_connect[0])),\n",
    "                         output_shape=shapel1, k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                         name='dec_deconv1_2', reuse=reuse))\n",
    "        xtp1 = tanh(deconv2d(deconv1_2, output_shape=shapeout1, k_h=3, k_w=3,\n",
    "                             d_h=1, d_w=1, name='dec_deconv1_1', reuse=reuse))\n",
    "        return xtp1\n",
    "\n",
    "    def discriminator(self, image):\n",
    "        h0 = lrelu(conv2d(image, self.df_dim, name='dis_h0_conv'))\n",
    "        h1 = lrelu(batch_norm(conv2d(h0, self.df_dim*2, name='dis_h1_conv'),\n",
    "                              \"bn1\"))\n",
    "        h2 = lrelu(batch_norm(conv2d(h1, self.df_dim*4, name='dis_h2_conv'),\n",
    "                              \"bn2\"))\n",
    "        h3 = lrelu(batch_norm(conv2d(h2, self.df_dim*8, name='dis_h3_conv'),\n",
    "                              \"bn3\"))\n",
    "        h = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'dis_h3_lin')\n",
    "\n",
    "        return tf.nn.sigmoid(h), h\n",
    "\n",
    "    def save(self, sess, checkpoint_dir, step):\n",
    "        model_name = \"MCNET.model\"\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    def load(self, sess, checkpoint_dir, model_name=None):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            if model_name is None: model_name = ckpt_name\n",
    "            self.saver.restore(sess, os.path.join(checkpoint_dir, model_name))\n",
    "            print(\" Loaded model: \"+str(model_name))\n",
    "            return True, model_name\n",
    "        else:\n",
    "            return False, None\n",
    "        \n",
    "    \n",
    "    # Execute the forward and the backward pass\n",
    "    def run_single_step(self,global_step):\n",
    "        print (\"global_step:\",global_step)\n",
    "        try:\n",
    "            train_batch = self.sess.run(self.train_iterator.get_next())\n",
    "            x = self.sess.run([self.x], feed_dict={self.x: train_batch[\"images\"]})\n",
    "            _, g_sum = self.sess.run([self.g_optim, self.g_sum],feed_dict={self.x: train_batch[\"images\"]})\n",
    "            _, d_sum = self.sess.run([self.d_optim, self.d_sum],feed_dict={self.x: train_batch[\"images\"]})\n",
    "            \n",
    "            gen_data,train_total_loss = self.sess.run([self.gen_data,self.total_loss],feed_dict={self.x: train_batch[\"images\"]})\n",
    "            #print(\"gen_data:\",gen_data[0][0])\n",
    "            #_, train_summary, d_losses = self.sess.run([self.d_optim, self.summary_op, self.d_loss], feed_dict={self.x: train_batch[\"images\"]})\n",
    "           # _, train_summary, g_losses = self.sess.run([self.g_optim, self.summary_op, self.L_GAN], feed_dict={self.x: train_batch[\"images\"]})\n",
    "            #self.train_writer.add_summary(train_summary, global_step)\n",
    "    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"train out of range error\")\n",
    "\n",
    "        try:\n",
    "            val_batch = self.sess.run(self.val_iterator.get_next())\n",
    "            val_total_loss = self.sess.run([self.total_loss], feed_dict={self.x: val_batch[\"images\"]})\n",
    "            #self.val_writer.add_summary(val_summary, global_step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "             print(\"train out of range error\")\n",
    "        \n",
    "        return train_total_loss, val_total_loss\n",
    "\n",
    "    \n",
    "    def trainer(self):        \n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            global_step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\n",
    "            sess = tf.Session()\n",
    "            print(\"Restore from {}\".format(ckpt.model_checkpoint_path))\n",
    "            #graph = tf.get_default_graph()\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            #saver = tf.train.import_meta_graph(os.path.join(model.checkpoint_dir,'model.ckpt-{}.meta'.format(global_step)))\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(self.checkpoint_dir))\n",
    "            #loaded_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        else:\n",
    "            print(\"Initializer from scratch\")\n",
    "            global_step = self.sess.run(self.global_step)\n",
    "           # Training loop\n",
    "            model.sess.run(self.train_iterator.initializer)\n",
    "            model.sess.run(self.val_iterator.initializer)\n",
    "        for epoch in range(num_epoch):\n",
    "            start_time = time.time()\n",
    "            # Run an epoch\n",
    "            for iter in range(num_sample // self.batch_size):\n",
    "                print(\"iter\",iter)\n",
    "                global_step = self.sess.run(self.global_step)\n",
    "                train_total_loss, val_total_loss = self.run_single_step(global_step)\n",
    "                print (\"Train_loss: {}; Val_loss{} for global step {}\".format(train_total_loss,val_total_loss,global_step))\n",
    "            end_time = time.time()\n",
    "        print('Done!')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnet_model= model_class()\n",
    "model_vae2 = mcnet_model.trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
